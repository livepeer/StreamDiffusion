# StreamDiffusion IPAdapter Video Test Configuration
# Optimized for real-time video processing with IPAdapter style conditioning

# Base model configuration
model_id: "runwayml/stable-diffusion-v1-5"
device: "cuda"
dtype: "float16"
width: 512
height: 512
mode: "img2img"  # img2img mode for video processing

# StreamDiffusion parameters optimized for video
t_index_list: [22, 32, 45]  # Different t_index_list for img2img (closer to img2img examples)
frame_buffer_size: 1
warmup: 10
acceleration: "xformers"
use_denoising_batch: true    # img2img typically uses denoising batch
cfg_type: "self"             # img2img uses "self" instead of "none"
seed: 42

# Text prompts (combined with IPAdapter style conditioning)
prompt: "beautiful cinematic lighting, high quality, detailed"
negative_prompt: "blurry, low quality, distorted, ugly"
num_inference_steps: 50  # Reduced for real-time performance
guidance_scale: 1.2
delta: 0.5

# Similar image filter for video stability
enable_similar_image_filter: true
similar_image_filter_threshold: 0.95
similar_image_filter_max_skip_frame: 5

# IPAdapter configuration for video processing
ipadapters:
  - ipadapter_model_path: "h94/IP-Adapter"  # Auto-downloads ip-adapter_sd15.bin
    image_encoder_path: "h94/IP-Adapter"    # Auto-downloads image_encoder
    style_image: "C:\\_dev\\comfy\\ComfyUI\\StreamDiffusion\\images\\inputs\\input.png"  # Style reference for video processing
    scale: 1.0  # Strong IPAdapter influence (same as "strong" output in reference script)
    enabled: true

# Engine settings for TensorRT optimization
engine_dir: "engines/ipadapter_video"
use_tiny_vae: true  # Faster VAE for video processing
use_lcm_lora: true  # Use LCM LoRA for speed 