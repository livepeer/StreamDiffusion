#!/usr/bin/env python3
"""
Test script for baked-in IPAdapter TensorRT implementation

This script tests that:
1. IPAdapter processors are correctly baked into the UNet during ONNX export
2. No separate image_embeddings input is needed 
3. Concatenated embeddings work correctly in both PyTorch and TensorRT modes
4. The implementation is backward compatible
"""

import os
import sys
import torch
from pathlib import Path
from PIL import Image

# Add StreamDiffusion to path
current_dir = Path(__file__).parent
streamdiffusion_path = current_dir / "src"
sys.path.insert(0, str(streamdiffusion_path))

def test_ipadapter_wrapper_baked_in():
    """Test that IPAdapter wrapper uses baked-in processors approach"""
    print("Test 1: IPAdapter wrapper with baked-in processors")
    
    try:
        from streamdiffusion.acceleration.tensorrt.ipadapter_wrapper import create_ipadapter_wrapper
        from diffusers import UNet2DConditionModel
        
        # Create a mock UNet
        unet = UNet2DConditionModel.from_pretrained(
            "runwayml/stable-diffusion-v1-5", 
            subfolder="unet",
            torch_dtype=torch.float32
        )
        
        # Test the wrapper
        wrapped_unet = create_ipadapter_wrapper(unet, num_tokens=4)
        
        # Verify it's baking in processors (not using separate inputs)
        print(f"Wrapper class: {wrapped_unet.__class__.__name__}")
        print(f"Number of tokens: {wrapped_unet.num_image_tokens}")
        print(f"Cross attention dim: {wrapped_unet.cross_attention_dim}")
        
        # Test forward pass with concatenated embeddings
        batch_size = 2
        sample = torch.randn(batch_size, 4, 64, 64, dtype=torch.float32)
        timestep = torch.randint(0, 1000, (batch_size,), dtype=torch.float32)
        
        # Concatenated embeddings (text + image tokens)
        text_tokens = 77
        image_tokens = 4
        total_tokens = text_tokens + image_tokens
        encoder_hidden_states = torch.randn(batch_size, total_tokens, 768, dtype=torch.float32)
        
        print(f"Testing forward pass:")
        print(f"  Sample shape: {sample.shape}")
        print(f"  Timestep shape: {timestep.shape}")
        print(f"  Encoder hidden states shape: {encoder_hidden_states.shape}")
        print(f"  (Contains {text_tokens} text tokens + {image_tokens} image tokens)")
        
        # This should work with baked-in processors
        with torch.no_grad():
            output = wrapped_unet(sample, timestep, encoder_hidden_states)
            print(f"  Output shape: {output[0].shape}")
            print("‚úì Forward pass successful with concatenated embeddings")
        
        return True
    except Exception as e:
        print(f"‚úó Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_tensorrt_models_no_separate_inputs():
    """Test that TensorRT UNet model doesn't expect separate image_embeddings"""
    print("\nTest 2: TensorRT UNet model input specifications")
    
    try:
        from streamdiffusion.acceleration.tensorrt.models import UNet
        
        # Test without IPAdapter
        unet_model_normal = UNet(
            embedding_dim=768,
            use_ipadapter=False
        )
        
        normal_inputs = unet_model_normal.get_input_names()
        print(f"Normal UNet inputs: {normal_inputs}")
        
        # Test with IPAdapter (baked-in)
        unet_model_ipadapter = UNet(
            embedding_dim=768,
            use_ipadapter=True,
            num_image_tokens=4
        )
        
        ipadapter_inputs = unet_model_ipadapter.get_input_names()
        print(f"IPAdapter UNet inputs: {ipadapter_inputs}")
        
        # Both should have the same inputs (no separate image_embeddings)
        assert normal_inputs == ipadapter_inputs, f"Input names should be the same, got {normal_inputs} vs {ipadapter_inputs}"
        
        # Verify no image_embeddings input
        assert "image_embeddings" not in ipadapter_inputs, "Should not have separate image_embeddings input"
        
        # Test that text_maxlen is extended for IPAdapter
        print(f"Normal text_maxlen: {unet_model_normal.text_maxlen}")
        print(f"IPAdapter text_maxlen: {unet_model_ipadapter.text_maxlen}")
        
        expected_maxlen = 77 + 4  # text tokens + image tokens
        assert unet_model_ipadapter.text_maxlen == expected_maxlen, f"Expected text_maxlen {expected_maxlen}, got {unet_model_ipadapter.text_maxlen}"
        
        print("‚úì TensorRT model correctly configured for baked-in IPAdapter")
        return True
    except Exception as e:
        print(f"‚úó Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_engine_no_image_embeddings():
    """Test that TensorRT engine doesn't expect image_embeddings parameter"""
    print("\nTest 3: TensorRT engine interface")
    
    try:
        from streamdiffusion.acceleration.tensorrt.engine import UNet2DConditionModelEngine
        import inspect
        
        # Check the engine __call__ method signature
        sig = inspect.signature(UNet2DConditionModelEngine.__call__)
        params = list(sig.parameters.keys())
        
        print(f"Engine __call__ parameters: {params}")
        
        # Should not have image_embeddings parameter
        assert "image_embeddings" not in params, f"Engine should not have image_embeddings parameter"
        
        # Should have the basic parameters
        assert "latent_model_input" in params
        assert "timestep" in params
        assert "encoder_hidden_states" in params
        
        print("‚úì TensorRT engine interface correctly updated for baked-in approach")
        return True
    except Exception as e:
        print(f"‚úó Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_pipeline_embedding_concatenation():
    """Test that IPAdapter pipeline still uses concatenation (same for both modes)"""
    print("\nTest 4: IPAdapter pipeline embedding handling")
    
    try:
        from streamdiffusion.ipadapter.base_ipadapter_pipeline import BaseIPAdapterPipeline
        import inspect
        
        # Check the _update_stream_embeddings method
        source = inspect.getsource(BaseIPAdapterPipeline._update_stream_embeddings)
        
        # Should still use concatenation (works for both PyTorch and TensorRT)
        assert "torch.cat" in source, "Should still concatenate embeddings"
        assert "IPAdapter mode - concatenating embeddings (same for TensorRT and PyTorch)" in source, "Should have updated comments for baked-in approach"
        
        print("‚úì Pipeline correctly handles embedding concatenation for both modes")
        return True
    except Exception as e:
        print(f"‚úó Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Run all tests for baked-in IPAdapter implementation"""
    print("Testing Baked-in IPAdapter TensorRT Implementation")
    print("=" * 60)
    
    tests = [
        test_ipadapter_wrapper_baked_in,
        test_tensorrt_models_no_separate_inputs,
        test_engine_no_image_embeddings,
        test_pipeline_embedding_concatenation,
    ]
    
    passed = 0
    failed = 0
    
    for test in tests:
        try:
            if test():
                passed += 1
            else:
                failed += 1
        except Exception as e:
            print(f"‚úó Test {test.__name__} crashed: {e}")
            failed += 1
    
    print("\n" + "=" * 60)
    print(f"Test Results: {passed} passed, {failed} failed")
    
    if failed == 0:
        print("üéâ All tests passed! Baked-in IPAdapter implementation is ready.")
        print("\nKey Changes Verified:")
        print("‚úì IPAdapter processors are baked into ONNX export")
        print("‚úì No separate image_embeddings input needed")
        print("‚úì Concatenated embeddings work in both PyTorch and TensorRT")
        print("‚úì Backward compatible with existing pipeline code")
        print("\nNext Steps:")
        print("- Test with actual TensorRT engine compilation")
        print("- Benchmark performance vs PyTorch IPAdapter")
        print("- Test with different IPAdapter variants (standard vs plus)")
    else:
        print(f"‚ùå {failed} tests failed. Implementation needs fixes.")
    
    return failed == 0

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 