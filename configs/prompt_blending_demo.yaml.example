# StreamDiffusion Configuration for Prompt & Seed Blending Demo
# Simple img2img setup without ControlNets for demonstrating prompt and seed blending

model_id: "KBlueLeaf/kohaku-v2.1"
t_index_list: [16, 32]
width: 512
height: 512
device: "cuda"
dtype: "float16"
mode: img2img

# Generation parameters (base - will be overridden by blending)
prompt: "a waifu girl cute"
negative_prompt: "blurry, low quality, ugly"
guidance_scale: 1.2
num_inference_steps: 50

# StreamDiffusion parameters
use_denoising_batch: true
delta: 0.7
frame_buffer_size: 1

# Pipeline configuration
pipeline_type: "sd1.5"
use_lcm_lora: true
use_tiny_vae: true
acceleration: "xformers"  # Use xformers instead of tensorrt for easier setup
cfg_type: "self"
seed: 42

# Warmup iterations for performance
warmup: 5

# Prompt blending configuration
# This will override the single 'prompt' above
# prompt_blending:
#   prompt_list:
#     - ["a waifu girl cute", 1.0]
#     - ["a demon from hell", 0.0]
#   interpolation_method: "slerp"  # or "linear"
#   enable_caching: true

# Seed blending configuration
# This enables blending between different noise patterns
# for added visual variety alongside prompt blending
seed_blending:
  seed_list:
    - [42, 1.0]      # Stable, controlled generation
    - [999, 0.0]     # More chaotic, varied generation
  interpolation_method: "linear"  # or "slerp"
  enable_caching: true

# No ControlNets for this demo
# controlnets: [] 